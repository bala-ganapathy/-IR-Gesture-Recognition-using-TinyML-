# âœ‹ IR Gesture Recognition using TinyML (ESP32 + Python)

A real-time gesture classification system built using an IR sensor and ESP32. This project detects and classifies three types of human gestures â€” **pass**, **stay**, and **wave** â€” using TinyML models trained on custom time-series sensor data.

It includes both:
- âœ… Local testing using Python + TensorFlow Lite
- âœ… On-device deployment using ESP32 with a Random Forest model (exported via EverywhereML)

---

## ğŸ“˜ Problem Statement & Project Flow

**Problem:**  
Detecting gesture types like **passing**, **pausing**, or **waving** is challenging using simple analog sensors. Traditional methods can't distinguish between these movements without complex camera systems or large models.

**Goal:**  
To build a compact, accurate, and real-time gesture recognition system using:
- Simple IR analog sensor  
- Lightweight ML model (Random Forest)  
- Real-time inference on **ESP32**

**Approach Flow:**  
IR Sensor â†’ ESP32 (ADC Readings) â†’ Model Input (30 values) â†’  
Local Inference (Python) / On-Device Inference (C++) â†’ Gesture Output

---

## ğŸ§ª Hardware & Connections

- **Microcontroller:** ESP32 Dev Module  
- **Sensor:** IR analog sensor (e.g., TSOP or similar)  
- **Connections:**  
  - Sensor OUT â†’ GPIO34 (analog input)  
  - GND and VCC to ESP32  
- **Serial Connection:** USB to Serial @ 9600 baud

---

## ğŸ“Š Data Collection

- Collected 30 analog samples every 100 ms (3-second window)
- Labeled 3 gestures:
  - `pass` â€“ quick motion past sensor
  - `stay` â€“ steady pause in front of sensor
  - `wave` â€“ alternating high/low signals
- Stored in `gestures.csv`  
- Each sample row: `label, f1, f2, ..., f30`  
- Values normalized to [0, 1]

---

## ğŸ’» Model Training & Export

- Training script: `train_gesture_model.py`
- Library: [`everywhereml`](https://github.com/eloquentarduino/everywhereml)
- Algorithm: `RandomForestClassifier` (10 trees)
- Dataset split: 80% train / 20% test
- Exported model as `model.h`

Achieved **~90.91% accuracy**  
Model runs directly on microcontroller as C++ code â€” no conversion or TFLite required.

---

## ğŸ§  Local Inference (Python + TFLite)

Used to test the `.tflite` model before deployment:

- Script: `predict_ir_gesture.py`
- Model: `ir_gesture_model.tflite`
- ESP32 sends 30 IR readings via Serial (COMx, 115200 baud)
- Python receives data, runs inference, and prints predictions

**Example output:**
Prediction:
pass: 3.21%
stay: 91.88%
wave: 4.91%

---

## ğŸ› ï¸ Deployment on ESP32

- Sketch: `gesture_model.ino`
- Reads analog input from GPIO34
- Feeds 30 readings into `model.predict(input)`
- Outputs predicted gesture (`pass`, `stay`, or `wave`) via Serial

Runs every 3â€“5 seconds completely offline using minimal memory.

---

---

## ğŸ“š Dependencies

| Environment | Dependencies |
|-------------|--------------|
| PC (Python) | numpy, pyserial, tensorflow, everywhereml |
| ESP32       | Arduino IDE only (no external libraries) |
| Hardware    | ESP32 Dev Module, IR analog sensor |

---

## ğŸ“ˆ Performance

- **Training Accuracy:** ~90.91%  
- **Model:** Random Forest with 10 estimators  
- **Inference Time:** Near-instant on ESP32  
- **Sampling:** 30 values Ã— 100 ms interval (3 seconds)

---

## ğŸ’¡ Future Enhancements

- Add OLED display or LED indicators for gesture feedback
- Add gesture-based triggers (e.g., control lights, audio)
- Train model for more gesture types
- Use Edge Impulse for larger-scale retraining and deployment

---

## ğŸ“‚ How to Run This Project

1. Clone this repository
2. Install dependencies:
   - Python: `numpy`, `pyserial`, `tensorflow`, `everywhereml`
3. Train the model using `train_gesture_model.py`
4. Test the model locally with `predict_ir_gesture.py`
5. Upload `gesture_model.ino` to ESP32 using Arduino IDE
6. Open Serial Monitor @ 9600 baud to view predictions

---

